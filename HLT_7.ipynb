{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HLT 7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMd06HQG4HungyqoSmF+OgE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alademk65/TTA_alademk65/blob/main/HLT_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Responsible AI\n",
        "According to Brookings, many stakeholders from government, industry, civil society, and academic institutions have embraced the concepts of ethical and accountable artificial intelligence (AI), sometimes known as \"responsible AI.\" Making AI systems visible, fair, safe, and inclusive are central to commonly stated responsible AI frameworks, but how they are perceived and operationalized by each organisation varies. Furthermore, there is significant discussion about whether responsible AI frameworks can address the explicit and implicit biases entrenched within algorithms to ensure fairness in prediction judgments, particularly when used to employment, health care, financial services, and criminal justice.\n",
        "\n",
        "Accenture also described responsible AI is the process of designing, developing, and deploying AI with the goal of empowering people and organisations while also having a fair influence on consumers and society, allowing businesses to build trust and expand AI with confidence."
      ],
      "metadata": {
        "id": "5I_Ip2WDJ8LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find instances where AI has failed? Or been used maliciously or incorrectlly\n",
        "\n",
        " \n",
        ">According to Immuniweb, Amazon wants to automate their recruiting process in order to shorten the candidate selection process for thousands of open positions. Amazon offers the following:\n",
        "\"They actually wanted it to be an engine where I give you 100 resumes, it spits out the best five, and we hire those.\" - said one of Amazon's developers on the proposal\".\n",
        "Everything ended up being a PR catastrophe since the system was discovered to be sexist, favouring white men. Most likely, the training data utilised to create the model was unequal, resulting in candidate selection bias.\n",
        "\n",
        "\n",
        "\n",
        ">In addition, a UK-based energy company's CEO received a call from his German employer asking him to transfer €220,000 ($243,000) to a Hungarian supplier. The 'boss' stated that the request was urgent and asked the UK CEO to send the funds as soon as possible.\n",
        "Unfortunately, the boss was a 'deep fake' speech generating programme that precisely replicated the genuine human's voice. According to The Wall Street Journal, it employed machine learning to become indistinguishable from the original, including the \"slight German accent and the melody of his voice.\""
      ],
      "metadata": {
        "id": "vDjHnyaOKLbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Implications of when AI fails. There is a specific article in the GDPR Law that covers this, especially with automated decision making. (opt in and out options).\n",
        "\n",
        " According to a publication by CMS, an autonomous automobile hits a pedestrian; a drone partially piloted by a pilot crashes and causes damage; and an AI software application incorrectly diagnoses medical therapy. In each of these scenarios, who would be held accountable? Who is the car's owner? Who created the medical software programme? Is the drone's manufacturer or pilot? Who created the AI system? What about the AI system itself?.Since there are many parties involved in an AI system  (data provider, designer, manufacturer, programmer, developer, user and AI system itself),liability is difficult to establish when something goes wrong.\n",
        "\n",
        "The article revealed that in the United Kingdom, for example, laws are being proposed under which the insurer will normally carry main liability in the event of an accident caused by an autonomous vehicle. In the absence of AI-related laws, people who have been harmed as a result of a failure of AI would most likely seek remedy under the tort of negligence.\n",
        "\n",
        "The claimant would have to prove that the defendant (whoever that may be) owed a duty of care, broke that duty, and caused the claimant harm. Liability for carelessness would ultimately fall on the person, individuals, or entities that caused the harm or defect or who might have predicted the product being used in the manner that it was used. If the damage is caused by AI system behaviours that were completely unexpected, this might be troublesome for negligence claims since a lack of foreseeability could result in no one being held accountable at all.\n",
        "\n",
        "As things stand, the user of an AI system is less likely than the manufacturer to be at fault. The applicable industry standards of care and whether the specifications were suitable in light of those standards will determine whether a manufacturer is responsible. There may be more discussions about whether or not the programmer, designer, or expert who contributed the information to the AI system is to blame, and to what degree. It's also possible that contributory negligence is a factor.\n",
        "\n",
        "Conclusively, it will be more challenging to build closeness and foreseeability if an AI system is entirely autonomous or far distant from human decision-making. In such circumstances, there is likely to be a lot of conflicting expert evidence about whether the AI system worked as it should have. An instance is the  class action lawsuit was filed against Tesla in early 2017 over the autopilot system of an autonomous car, alleging that it contains inoperative safety features and fault improvements."
      ],
      "metadata": {
        "id": "guJVvdNWKX5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  What should organisations do to ensure that they are being responsible with AI and the wider use of data in general?\n",
        "\n",
        "According to Microsoft Industry Blogs, organizations must approach AI technology holistically, recognising where AI fits in the value chain and establishing the necessary mechanisms to guarantee long-term governance by:\n",
        "  \n",
        "> Establishing internal governance, for example, through the formation of a \n",
        "broad and objective review panel with the ability to grasp the potential repercussions of AI-infused systems. Leadership support and the ability to hold leaders responsible are critical success factors.\n",
        "\n",
        "> Ensuring proper technological safeguards, quality assurance, and governance in order to ensure traceability and auditability for AI systems. This is an essential component of any organization's toolset for allowing operational and responsible AI to scale.\n",
        "\n",
        "> Investing more in their own AI education and training to ensure that all stakeholders – internal and external – are aware of AI possibilities as well as potential hazards.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iQdjiIhqKmHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        ">https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework-2020 \n",
        "\n",
        ">https://www.accenture.com/gb-en/insights/artificial-intelligence/responsible-ai-principles-practice\n",
        "\n",
        ">https://www.brookings.edu/events/what-is-responsible-ai/\n",
        "\n",
        ">https://www.immuniweb.com/blog/top-10-failures-of-ai.html\n",
        "\n",
        ">https://blog.f-secure.com/malicious-use-of-ai/\n",
        "\n",
        ">https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/individual-rights/rights-related-to-automated-decision-making-including-profiling/\n",
        "\n",
        "\n",
        ">https://cloudblogs.microsoft.com/industry-blog/en-gb/cross-industry/2020/01/08/3-ways-organisations-can-use-ai-in-a-responsible-way/#:~:text=Organisations%20must%20think%20of%20AI,ensure%20long%2Dterm%20governance%20by%3A&text=Ensuring%20the%20right%20technical%20guardrails,and%20auditability%20for%20AI%20systems.\n",
        "\n",
        ">https://syncedreview.com/2021/01/01/2020-in-review-10-ai-failures/\n",
        "\n",
        ">https://cms.law/en/gbr/publication/artificial-intelligence-who-is-liable-when-ai-fails-to-perform"
      ],
      "metadata": {
        "id": "sMH3xTEI6bpC"
      }
    }
  ]
}